{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransBE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 scRNA-seq DataArry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.io import mmread\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def mtx2csv(Dir=\"./\",Output=\"./Output.csv.gz\"):\n",
    "    Data = mmread(os.path.join(Dir, \"matrix.mtx\"))\n",
    "    Gene_Name = pd.read_table(os.path.join(Dir, \"genes.tsv\"),header=None)\n",
    "    Cell_Name = pd.read_table(os.path.join(Dir, \"barcodes.tsv\"),header=None)\n",
    "    Matrix = pd.DataFrame(Data.toarray().T, index=Cell_Name[0].to_list(), columns=Gene_Name[1].to_list())\n",
    "    Matrix.to_csv(Output, compression='gzip')\n",
    "\n",
    "mtx2csv(Dir=\"../../../Experiment/ICC-Data/RAW/GSE125449/S2/\",Output=\"../Data/Sample2.csv.gz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 不同批次数据比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def Classification(filepath):\n",
    "    Data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    adata = sc.read(filepath)\n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    sc.pp.filter_cells(adata, max_genes=30000)\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "    adata = adata[adata.obs.n_genes_by_counts < 2500, :]\n",
    "    adata = adata[adata.obs.pct_counts_mt < 5, :]\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    sc.pp.scale(adata, max_value=10)\n",
    "    sc.tl.pca(adata)\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.leiden(adata,resolution=0.95)\n",
    "    \n",
    "    Leiden = pd.DataFrame(adata.obs[\"leiden\"])\n",
    "    Data = pd.concat([Data, Leiden], axis=1, join=\"inner\")\n",
    "    return Data\n",
    "\n",
    "Data1 = Classification(\"../Data/Sample1.csv.gz\")\n",
    "Data2 = Classification(\"../Data/Sample2.csv.gz\")\n",
    "Keys = list(set(Data1.columns) & set(Data2.columns))\n",
    "Data1 = Data1[Keys]\n",
    "Data2 = Data2[Keys]\n",
    "Data1 = Data1.sort_values(\"leiden\")\n",
    "Data2 = Data2.sort_values(\"leiden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2864/2864 [00:01<00:00, 1497.52it/s]\n",
      "100%|██████████| 3893/3893 [02:58<00:00, 21.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# > Calculate Loss\n",
    "#   1. Identify the nearest cluster\n",
    "#   2. calculate distences\n",
    "#   3. inner divided outer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def Calculate_Distance(A, B):\n",
    "    # A is a matrix which size is (m, k)\n",
    "    # B is a matrix which size is (n, k)\n",
    "    # D is a matrix which size is (m, n)\n",
    "    m = np.shape(A)[0]\n",
    "    n = np.shape(B)[0]\n",
    "    D = np.zeros((m,n))\n",
    "\n",
    "    M = np.dot(A, B.T)\n",
    "    H1 = np.square(A).sum(axis=1) # (1, n) Vector\n",
    "    H2 = np.square(B).sum(axis=1) # (1, m) Vector\n",
    "    D = np.sqrt(-2*M + np.matrix(H2) + np.matrix(H1).T)\n",
    "    return D\n",
    "\n",
    "def Calculate_PCA(Data, N):\n",
    "    pca = PCA(n_components=N)\n",
    "    pca_results = pca.fit(Data)\n",
    "    D = pca.fit_transform(Data)\n",
    "    return D\n",
    "\n",
    "def Calculate_Normalize(Matrix):\n",
    "    # Normalize\n",
    "    for i, index in enumerate(tqdm(range(Matrix.shape[0]))):\n",
    "        Line = Matrix.iloc[index,:]\n",
    "        Min = min(Line.to_list())\n",
    "        Max = max(Line.to_list())\n",
    "        Matrix.iloc[index,:] = (Line - Min) / (Max-Min)\n",
    "    return Matrix\n",
    "\n",
    "def Calculate_Loss(Data1, Data2):\n",
    "    Data_All = pd.concat([Data1.drop([\"leiden\"], axis=1), Data2.drop([\"leiden\"], axis=1)])\n",
    "    Data_All = Calculate_PCA(Data_All, N=50)\n",
    "    D1 = Data_All[0:Data1.shape[0], :]\n",
    "    D2 = Data_All[Data1.shape[0]:, :]\n",
    "    Distance_Matrix = Calculate_Distance(D1, D2)\n",
    "    Distance_Matrix = pd.DataFrame(Distance_Matrix)\n",
    "    Distance_Matrix = Distance_Matrix.set_index(Data1.index)\n",
    "    Distance_Matrix.columns = Data2.index\n",
    "    Distance_Matrix = Calculate_Normalize(Distance_Matrix)\n",
    "\n",
    "    Loss_list = []\n",
    "    for i, Barcode2 in enumerate(tqdm(list(Distance_Matrix.columns))):\n",
    "        Closest_point = Distance_Matrix[Barcode2].sort_values().index[0]\n",
    "        c1 = Data1.loc[Closest_point, \"leiden\"]\n",
    "        Barcode1 = list(Data1[Data1[\"leiden\"]==c1].index)\n",
    "        Distance_In_Group = Distance_Matrix.loc[Barcode1, Barcode2].mean()\n",
    "        Distance_Out_Group = Distance_Matrix.drop(Barcode1)[Barcode2].mean()\n",
    "        Loss = Distance_In_Group/Distance_Out_Group\n",
    "        Loss_list.append(Loss)\n",
    "    Loss_all = np.mean(Loss_list)\n",
    "    return Loss_all\n",
    "\n",
    "Loss_all = Calculate_Loss(Data1, Data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19747024703453758"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 GPU实现计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2864/2864 [00:00<00:00, 4603.94it/s]\n",
      "100%|██████████| 3894/3894 [00:01<00:00, 2037.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1941, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def Calculate_PCA(Data, N):\n",
    "    pca = PCA(n_components=N)\n",
    "    pca_results = pca.fit(Data)\n",
    "    D = pca.fit_transform(Data)\n",
    "    return D\n",
    "\n",
    "def Calculate_Distance(A, B):\n",
    "    m = A.shape[0]\n",
    "    n = B.shape[0]\n",
    "    D = torch.zeros([m ,n])\n",
    "\n",
    "    M = torch.matmul(A, B.T)\n",
    "    H1 = torch.sum(torch.square(A), axis=1).reshape(1,-1)\n",
    "    H2 = torch.sum(torch.square(B), axis=1).reshape(1,-1)\n",
    "    D = torch.sqrt(-2*M + H2 + H1.T)\n",
    "    return D\n",
    "\n",
    "def Calculate_Normalize(Matrix):\n",
    "    # Normalize\n",
    "    for i, index in enumerate(tqdm(range(Matrix.shape[0]))):\n",
    "        Line = Matrix[index,:]\n",
    "        Min = torch.min(Line)\n",
    "        Max = torch.max(Line)\n",
    "        Matrix[index,:] = (Line - Min) / (Max-Min)\n",
    "    return Matrix\n",
    "\n",
    "\n",
    "\n",
    "Data_All = pd.concat([Data1.drop([\"leiden\"], axis=1), Data2.drop([\"leiden\"], axis=1)])\n",
    "Data_All = Calculate_PCA(Data_All, N=50)\n",
    "D1 = Data_All[0:Data1.shape[0], :]\n",
    "D2 = Data_All[Data1.shape[0]:, :]\n",
    "D1 = torch.Tensor(D1).cuda()\n",
    "D2 = torch.Tensor(D2).cuda()\n",
    "Distance_Matrix = Calculate_Distance(D1, D2)\n",
    "Distance_Matrix = Calculate_Normalize(Distance_Matrix)\n",
    "Leiden1 = torch.tensor(list(map(int, Data1[\"leiden\"].to_list()))).cuda().reshape(-1,1)\n",
    "Leiden2 = torch.tensor(list(map(int, Data2[\"leiden\"].to_list())))\n",
    "Leiden2 = torch.cat((Leiden2, torch.tensor([-1])), 0).cuda().reshape(1,-1)\n",
    "Distance_Matrix = torch.cat((Distance_Matrix, Leiden1), 1)\n",
    "Distance_Matrix = torch.cat((Distance_Matrix, Leiden2), 0)\n",
    "\n",
    "i_max = Distance_Matrix.shape[0] - 1\n",
    "j_max = Distance_Matrix.shape[1] - 1\n",
    "Loss_list = torch.zeros([j_max+1]).cuda()\n",
    "Distance_Min_Index = torch.argmin(Distance_Matrix[:-1,:],dim=0)\n",
    "\n",
    "for i,Index in enumerate(tqdm(Distance_Min_Index)):\n",
    "    Cluster1 = Distance_Matrix[Index, -1]\n",
    "    Mask1 = (Distance_Matrix[:,-1] == Cluster1)\n",
    "    Distance_In_Group = torch.mean(Distance_Matrix[Mask1, i])\n",
    "    Distance_Out_Group = torch.mean(Distance_Matrix[Mask1 == False, i])\n",
    "    Loss_list[i] = Distance_In_Group/Distance_Out_Group\n",
    "Loss_All = torch.mean(Loss_list[:-1])\n",
    "print(Loss_All)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Transformer 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
