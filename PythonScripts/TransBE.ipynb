{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransBE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 scRNA-seq DataArry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.io import mmread\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def mtx2csv(Dir=\"./\",Output=\"./Output.csv.gz\"):\n",
    "    Data = mmread(os.path.join(Dir, \"matrix.mtx\"))\n",
    "    Gene_Name = pd.read_table(os.path.join(Dir, \"genes.tsv\"),header=None)\n",
    "    Cell_Name = pd.read_table(os.path.join(Dir, \"barcodes.tsv\"),header=None)\n",
    "    Matrix = pd.DataFrame(Data.toarray().T, index=Cell_Name[0].to_list(), columns=Gene_Name[1].to_list())\n",
    "    Matrix.to_csv(Output, compression='gzip')\n",
    "\n",
    "mtx2csv(Dir=\"../../../Experiment/ICC-Data/RAW/GSE125449/S2/\",Output=\"../Data/Sample2.csv.gz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 不同批次数据比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def Classification(filepath):\n",
    "    Data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    adata = sc.read(filepath)\n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    sc.pp.filter_cells(adata, max_genes=30000)\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "    adata = adata[adata.obs.n_genes_by_counts < 2500, :]\n",
    "    adata = adata[adata.obs.pct_counts_mt < 5, :]\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    sc.pp.scale(adata, max_value=10)\n",
    "    sc.tl.pca(adata)\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.leiden(adata,resolution=0.95)\n",
    "    \n",
    "    Leiden = pd.DataFrame(adata.obs[\"leiden\"])\n",
    "    Data = pd.concat([Data, Leiden], axis=1, join=\"inner\")\n",
    "    return Data\n",
    "\n",
    "Data1 = Classification(\"../Data/Sample1.csv.gz\")\n",
    "Data2 = Classification(\"../Data/Sample2.csv.gz\")\n",
    "Keys = list(set(Data1.columns) & set(Data2.columns))\n",
    "Data1 = Data1[Keys]\n",
    "Data2 = Data2[Keys]\n",
    "Data1 = Data1.sort_values(\"leiden\")\n",
    "Data2 = Data2.sort_values(\"leiden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2864/2864 [00:01<00:00, 1485.59it/s]\n",
      "100%|██████████| 3893/3893 [03:00<00:00, 21.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14008458996483159, 0.14624731649312428, 0.12371892944926068, 0.05176007254008147, 0.04574608256506818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# > Calculate Loss\n",
    "#   1. Identify the nearest cluster\n",
    "#   2. calculate distences\n",
    "#   3. inner divided outer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def Calculate_Distance(A, B):\n",
    "    # A is a matrix which size is (m, k)\n",
    "    # B is a matrix which size is (n, k)\n",
    "    # D is a matrix which size is (m, n)\n",
    "    m = np.shape(A)[0]\n",
    "    n = np.shape(B)[0]\n",
    "    D = np.zeros((m,n))\n",
    "\n",
    "    M = np.dot(A, B.T)\n",
    "    H1 = np.square(A).sum(axis=1) # (1, n) Vector\n",
    "    H2 = np.square(B).sum(axis=1) # (1, m) Vector\n",
    "    D = np.sqrt(-2*M + np.matrix(H2) + np.matrix(H1).T)\n",
    "    return D\n",
    "\n",
    "def Calculate_PCA(Data, N):\n",
    "    pca = PCA(n_components=N)\n",
    "    pca_results = pca.fit(Data)\n",
    "    D = pca.fit_transform(Data)\n",
    "    return D\n",
    "\n",
    "def Calculate_Normalize(Matrix):\n",
    "    # Normalize\n",
    "    for i, index in enumerate(tqdm(range(Matrix.shape[0]))):\n",
    "        Line = Matrix.iloc[index,:]\n",
    "        Min = min(Line.to_list())\n",
    "        Max = max(Line.to_list())\n",
    "        Matrix.iloc[index,:] = (Line - Min) / (Max-Min)\n",
    "    return Matrix\n",
    "\n",
    "def Calculate_Loss(Data1, Data2):\n",
    "    Data_All = pd.concat([Data1.drop([\"leiden\"], axis=1), Data2.drop([\"leiden\"], axis=1)])\n",
    "    Data_All = Calculate_PCA(Data_All, N=50)\n",
    "    D1 = Data_All[0:Data1.shape[0], :]\n",
    "    D2 = Data_All[Data1.shape[0]:, :]\n",
    "    Distance_Matrix = Calculate_Distance(D1, D2)\n",
    "    Distance_Matrix = pd.DataFrame(Distance_Matrix)\n",
    "    Distance_Matrix = Distance_Matrix.set_index(Data1.index)\n",
    "    Distance_Matrix.columns = Data2.index\n",
    "    Distance_Matrix = Calculate_Normalize(Distance_Matrix)\n",
    "\n",
    "    Loss_list = []\n",
    "    for i, Barcode2 in enumerate(tqdm(list(Distance_Matrix.columns))):\n",
    "        Closest_point = Distance_Matrix[Barcode2].sort_values().index[0]\n",
    "        c1 = Data1.loc[Closest_point, \"leiden\"]\n",
    "        Barcode1 = list(Data1[Data1[\"leiden\"]==c1].index)\n",
    "        Distance_In_Group = Distance_Matrix.loc[Barcode1, Barcode2].mean()\n",
    "        Distance_Out_Group = Distance_Matrix.drop(Barcode1)[Barcode2].mean()\n",
    "        Loss = Distance_In_Group/Distance_Out_Group\n",
    "        Loss_list.append(Loss)\n",
    "    Loss_all = np.mean(Loss_list)\n",
    "    print(Loss_list[0:5])\n",
    "    return Loss_all\n",
    "\n",
    "Loss_all = Calculate_Loss(Data1, Data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19747024703453758"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1400899589, 0.1462488025, 0.1237192824, 0.0517736003, 0.0457605310],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss_list[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 GPU实现计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2864/2864 [00:00<00:00, 4994.18it/s]\n",
      "100%|██████████| 3894/3894 [00:01<00:00, 2188.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1974757463, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "torch.set_printoptions(precision=10)\n",
    "\n",
    "\n",
    "def Calculate_PCA(Data, N):\n",
    "    pca = PCA(n_components=N)\n",
    "    pca_results = pca.fit(Data)\n",
    "    D = pca.fit_transform(Data)\n",
    "    return D\n",
    "\n",
    "def Calculate_Distance(A, B):\n",
    "    m = A.shape[0]\n",
    "    n = B.shape[0]\n",
    "    D = torch.zeros([m ,n])\n",
    "\n",
    "    M = torch.matmul(A, B.T)\n",
    "    H1 = torch.sum(torch.square(A), axis=1).reshape(1,-1)\n",
    "    H2 = torch.sum(torch.square(B), axis=1).reshape(1,-1)\n",
    "    D = torch.sqrt(-2*M + H2 + H1.T)\n",
    "    return D\n",
    "\n",
    "def Calculate_Normalize(Matrix):\n",
    "    # Normalize\n",
    "    for i, index in enumerate(tqdm(range(Matrix.shape[0]))):\n",
    "        Line = Matrix[index,:]\n",
    "        Min = torch.min(Line)\n",
    "        Max = torch.max(Line)\n",
    "        Matrix[index,:] = (Line - Min) / (Max-Min)\n",
    "    return Matrix\n",
    "\n",
    "\n",
    "\n",
    "Data_All = pd.concat([Data1.drop([\"leiden\"], axis=1), Data2.drop([\"leiden\"], axis=1)])\n",
    "Data_All = Calculate_PCA(Data_All, N=50)\n",
    "D1 = Data_All[0:Data1.shape[0], :]\n",
    "D2 = Data_All[Data1.shape[0]:, :]\n",
    "D1 = torch.FloatTensor(D1).cuda()\n",
    "D2 = torch.FloatTensor(D2).cuda()\n",
    "Distance_Matrix = Calculate_Distance(D1, D2)\n",
    "Distance_Matrix = Calculate_Normalize(Distance_Matrix)\n",
    "Leiden1 = torch.FloatTensor(list(map(int, Data1[\"leiden\"].to_list()))).cuda().reshape(-1,1)\n",
    "Leiden2 = torch.FloatTensor(list(map(int, Data2[\"leiden\"].to_list())))\n",
    "Leiden2 = torch.cat((Leiden2, torch.tensor([-1])), 0).cuda().reshape(1,-1)\n",
    "Distance_Matrix = torch.cat((Distance_Matrix, Leiden1), 1)\n",
    "Distance_Matrix = torch.cat((Distance_Matrix, Leiden2), 0)\n",
    "\n",
    "i_max = Distance_Matrix.shape[0] - 1\n",
    "j_max = Distance_Matrix.shape[1] - 1\n",
    "Loss_list = torch.zeros([j_max+1]).cuda()\n",
    "Distance_Min_Index = torch.argmin(Distance_Matrix[:-1,:],dim=0)\n",
    "\n",
    "for i,Index in enumerate(tqdm(Distance_Min_Index)):\n",
    "    Cluster1 = Distance_Matrix[Index, -1]\n",
    "    Mask1 = (Distance_Matrix[:,-1] == Cluster1)[:-1]\n",
    "    Distance_List = Distance_Matrix[:-1,i]\n",
    "    Distance_In_Group = torch.mean(Distance_List[Mask1])\n",
    "    Distance_Out_Group = torch.mean(Distance_List[Mask1 == False])\n",
    "    Loss_list[i] = Distance_In_Group/Distance_Out_Group\n",
    "Loss_All = torch.mean(Loss_list)\n",
    "print(Loss_All)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Transformer 模型搭建"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 输入模块"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab) -> None:\n",
    "        # d_model: 词嵌入维度\n",
    "        # vocab: 词汇总数\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Input Position Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, p_dropout=0.1, max_len=5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.pe = pe\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad = False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 编码器模块"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 掩码张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(attn_shape, k=1).astype(\"uint8\")\n",
    "    return torch.from_numpy(1 - subsequent_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 注意力机制\n",
    "计算规则 $Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_{model}}})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_model = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)/math.sqrt(d_model))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a530222350812ed26e4b37a93ea4484c289c17ae3cbdbcad04b3fa45ff755b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
