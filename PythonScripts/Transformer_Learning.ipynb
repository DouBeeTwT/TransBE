{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Transformer架构解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab) -> None:\n",
    "        # d_model: 词嵌入维度\n",
    "        # vocab: 词汇总数\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Result tensor([[[ 2.8251e+01,  1.2709e+01, -2.3858e+01,  ...,  5.2831e+01,\n",
      "           1.1292e+01, -2.4750e+00],\n",
      "         [ 5.5775e+01, -3.6318e+01, -9.9836e+00,  ...,  6.6456e+00,\n",
      "          -4.0601e+01, -6.6463e+00],\n",
      "         [-2.7960e+01,  2.2780e+00, -5.9441e+00,  ...,  1.1343e+00,\n",
      "           8.0652e+00, -2.5514e+01],\n",
      "         [-1.8473e+01,  5.5878e+00,  3.0744e+01,  ...,  8.9195e-01,\n",
      "          -1.6658e+01, -4.1357e+01]],\n",
      "\n",
      "        [[-3.6581e+01,  1.6614e+01,  1.9827e+01,  ...,  1.8057e+01,\n",
      "          -9.4189e+00, -6.0719e+00],\n",
      "         [ 4.2156e+01,  1.1574e+01,  2.1524e+00,  ..., -3.9088e-03,\n",
      "           1.4521e+01, -2.7413e+01],\n",
      "         [ 3.5479e+01,  2.8121e+01,  1.6331e+00,  ..., -2.0129e+01,\n",
      "           3.3834e+00,  8.4812e+00],\n",
      "         [-1.1242e+01, -1.4735e+01,  4.1057e+01,  ...,  6.9943e+00,\n",
      "           2.2428e+00, -2.8003e+01]]], grad_fn=<MulBackward0>)\n",
      "Embedding Result Shape torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "vocab = 1000\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508],[491, 998, 1, 221]]))\n",
    "emb = Embeddings(d_model, vocab)\n",
    "emb_result = emb(x)\n",
    "print(\"Embedding Result\", emb_result)\n",
    "print(\"Embedding Result Shape\", emb_result.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, p_dropout=0.1, max_len=5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad = False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE Result tensor([[[ 31.3905,  15.2318, -26.5090,  ...,  59.8119,   0.0000,  -1.6388],\n",
      "         [ 62.9070, -39.7534, -10.1797,  ...,   8.4951, -45.1120,  -6.2737],\n",
      "         [-30.0562,   2.0687,  -5.5641,  ...,   2.3714,   8.9616, -27.2383],\n",
      "         [-20.3684,   5.1087,  34.4318,  ...,   2.1022, -18.5091, -44.8415]],\n",
      "\n",
      "        [[-40.6450,  19.5714,  22.0305,  ...,  21.1747, -10.4654,  -5.6355],\n",
      "         [ 47.7747,  13.4607,   3.3047,  ...,   1.1068,  16.1348, -29.3475],\n",
      "         [ 40.4311,  30.7834,   2.8550,  ..., -21.2543,   0.0000,   0.0000],\n",
      "         [ -0.0000, -17.4720,  45.8916,  ...,   8.8826,   2.4923, -30.0030]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "PE Result Shape torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 60\n",
    "x = emb_result\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "print(\"PE Result\", pe_result)\n",
    "print(\"PE Result Shape\", pe_result.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 掩码张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\"uint8\")\n",
    "    return torch.from_numpy(1 - subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM tensor([[[1, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
      "SM Shape torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "sm = subsequent_mask(size)\n",
    "print(\"SM\", sm)\n",
    "print(\"SM Shape\", sm.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_model = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)/math.sqrt(d_model))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学习小结\n",
    "1. 什么是注意力计算规则"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 多头注意力机制\n",
    "\n",
    "多头注意力机制：只使用一组线性变换层，对三个变换张量Q,K,V分别进行线性变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # 深度copy工具包\n",
    "import torch.nn as nn\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, p_dropout=0.1) -> None:\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert embedding_dim % head == 0\n",
    "        self.d_k = embedding_dim // head\n",
    "        self.head = head\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        batch_size = query.size(0)\n",
    "        query, key, value = [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1,2) \n",
    "                             for model, x in zip(self.linears, (query, key, value))]\n",
    "        x, self.attn = attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 8 \n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(8,4,4))\n",
    "mha =MultiHeadedAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
